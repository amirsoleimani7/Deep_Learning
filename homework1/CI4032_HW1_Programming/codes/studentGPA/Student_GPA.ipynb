{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentID              0\n",
       "Age                    0\n",
       "Gender                 0\n",
       "Ethnicity              0\n",
       "ParentalEducation    142\n",
       "StudyTimeWeekly        0\n",
       "Absences               0\n",
       "Tutoring               0\n",
       "ParentalSupport      132\n",
       "Extracurricular        0\n",
       "Sports                 0\n",
       "Music                  0\n",
       "Volunteering           0\n",
       "GPA                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numeric fields \n",
    "- some of the fields like ParentalEducation , ParentalSupport should be in numeric form \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replaced ParentalEducation nulls with: Some College\n",
      "Replaced ParentalSupport nulls with: Moderate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7688/1684217187.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['ParentalEducation'].fillna(education_mode, inplace=True)\n",
      "/tmp/ipykernel_7688/1684217187.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['ParentalSupport'].fillna(support_mode, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StudentID            0\n",
       "Age                  0\n",
       "Gender               0\n",
       "Ethnicity            0\n",
       "ParentalEducation    0\n",
       "StudyTimeWeekly      0\n",
       "Absences             0\n",
       "Tutoring             0\n",
       "ParentalSupport      0\n",
       "Extracurricular      0\n",
       "Sports               0\n",
       "Music                0\n",
       "Volunteering         0\n",
       "GPA                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replacing the missing value with the most frequesnt ones in the dataset\n",
    "\n",
    "education_mode = df['ParentalEducation'].mode()[0]\n",
    "support_mode = df['ParentalSupport'].mode()[0]\n",
    "\n",
    "df['ParentalEducation'].fillna(education_mode, inplace=True)\n",
    "df['ParentalSupport'].fillna(support_mode, inplace=True)\n",
    "\n",
    "print(f\"\\nReplaced ParentalEducation nulls with: {education_mode}\")\n",
    "print(f\"Replaced ParentalSupport nulls with: {support_mode}\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorial to some numeric value\n",
    "\n",
    "df['ParentalEducation'] = pd.factorize(df['ParentalEducation'])[0]\n",
    "df['ParentalSupport'] = pd.factorize(df['ParentalSupport'])[0]\n",
    "df['Ethnicity'] = pd.factorize(df['Ethnicity'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffrentiat ethe values and stuff\n",
    "X = df.drop(['StudentID', 'GPA'], axis=1)\n",
    "y = df['GPA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set shapes:\n",
      "X_train: (1224, 12)\n",
      "y_train: (1224,)\n",
      "\n",
      "Testing set shapes:\n",
      "X_test: (306, 12)\n",
      "y_test: (306,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"\\nTraining set shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "\n",
    "print(\"\\nTesting set shapes:\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vonersion to PyTorch tensors \n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "#Create data loaders\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.layer4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImprovedNN(\n",
       "  (layer1): Linear(in_features=12, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (layer4): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = X_train_tensor.shape[1]\n",
    "model = ImprovedNN(input_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 1200\n",
    "patience = 1200\n",
    "best_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/2000], Loss: 0.2648\n",
      "Epoch [20/2000], Loss: 0.1780\n",
      "Epoch [30/2000], Loss: 0.1543\n",
      "Epoch [40/2000], Loss: 0.1286\n",
      "Epoch [50/2000], Loss: 0.1215\n",
      "Epoch [60/2000], Loss: 0.1152\n",
      "Epoch [70/2000], Loss: 0.1029\n",
      "Epoch [80/2000], Loss: 0.0921\n",
      "Epoch [90/2000], Loss: 0.0914\n",
      "Epoch [100/2000], Loss: 0.0900\n",
      "Epoch [110/2000], Loss: 0.0879\n",
      "Epoch [120/2000], Loss: 0.0816\n",
      "Epoch [130/2000], Loss: 0.0785\n",
      "Epoch [140/2000], Loss: 0.0771\n",
      "Epoch [150/2000], Loss: 0.0824\n",
      "Epoch [160/2000], Loss: 0.0728\n",
      "Epoch [170/2000], Loss: 0.0690\n",
      "Epoch [180/2000], Loss: 0.0667\n",
      "Epoch [190/2000], Loss: 0.0682\n",
      "Epoch [200/2000], Loss: 0.0710\n",
      "Epoch [210/2000], Loss: 0.0672\n",
      "Epoch [220/2000], Loss: 0.0664\n",
      "Epoch [230/2000], Loss: 0.0663\n",
      "Epoch [240/2000], Loss: 0.0602\n",
      "Epoch [250/2000], Loss: 0.0630\n",
      "Epoch [260/2000], Loss: 0.0600\n",
      "Epoch [270/2000], Loss: 0.0597\n",
      "Epoch [280/2000], Loss: 0.0654\n",
      "Epoch [290/2000], Loss: 0.0582\n",
      "Epoch [300/2000], Loss: 0.0599\n",
      "Epoch [310/2000], Loss: 0.0582\n",
      "Epoch [320/2000], Loss: 0.0557\n",
      "Epoch [330/2000], Loss: 0.0601\n",
      "Epoch [340/2000], Loss: 0.0570\n",
      "Epoch [350/2000], Loss: 0.0583\n",
      "Epoch [360/2000], Loss: 0.0593\n",
      "Epoch [370/2000], Loss: 0.0590\n",
      "Epoch [380/2000], Loss: 0.0561\n",
      "Epoch [390/2000], Loss: 0.0592\n",
      "Epoch [400/2000], Loss: 0.0543\n",
      "Epoch [410/2000], Loss: 0.0578\n",
      "Epoch [420/2000], Loss: 0.0559\n",
      "Epoch [430/2000], Loss: 0.0546\n",
      "Epoch [440/2000], Loss: 0.0566\n",
      "Epoch [450/2000], Loss: 0.0528\n",
      "Epoch [460/2000], Loss: 0.0531\n",
      "Epoch [470/2000], Loss: 0.0576\n",
      "Epoch [480/2000], Loss: 0.0536\n",
      "Epoch [490/2000], Loss: 0.0526\n",
      "Epoch [500/2000], Loss: 0.0549\n",
      "Epoch [510/2000], Loss: 0.0548\n",
      "Epoch [520/2000], Loss: 0.0526\n",
      "Epoch [530/2000], Loss: 0.0542\n",
      "Epoch [540/2000], Loss: 0.0508\n",
      "Epoch [550/2000], Loss: 0.0515\n",
      "Epoch [560/2000], Loss: 0.0513\n",
      "Epoch [570/2000], Loss: 0.0514\n",
      "Epoch [580/2000], Loss: 0.0485\n",
      "Epoch [590/2000], Loss: 0.0514\n",
      "Epoch [600/2000], Loss: 0.0522\n",
      "Epoch [610/2000], Loss: 0.0525\n",
      "Epoch [620/2000], Loss: 0.0526\n",
      "Epoch [630/2000], Loss: 0.0522\n",
      "Epoch [640/2000], Loss: 0.0496\n",
      "Epoch [650/2000], Loss: 0.0511\n",
      "Epoch [660/2000], Loss: 0.0489\n",
      "Epoch [670/2000], Loss: 0.0548\n",
      "Epoch [680/2000], Loss: 0.0490\n",
      "Epoch [690/2000], Loss: 0.0508\n",
      "Epoch [700/2000], Loss: 0.0499\n",
      "Epoch [710/2000], Loss: 0.0504\n",
      "Epoch [720/2000], Loss: 0.0528\n",
      "Epoch [730/2000], Loss: 0.0489\n",
      "Epoch [740/2000], Loss: 0.0503\n",
      "Epoch [750/2000], Loss: 0.0519\n",
      "Epoch [760/2000], Loss: 0.0515\n",
      "Epoch [770/2000], Loss: 0.0510\n",
      "Epoch [780/2000], Loss: 0.0464\n",
      "Epoch [790/2000], Loss: 0.0519\n",
      "Epoch [800/2000], Loss: 0.0497\n",
      "Epoch [810/2000], Loss: 0.0488\n",
      "Epoch [820/2000], Loss: 0.0476\n",
      "Epoch [830/2000], Loss: 0.0474\n",
      "Epoch [840/2000], Loss: 0.0492\n",
      "Epoch [850/2000], Loss: 0.0473\n",
      "Epoch [860/2000], Loss: 0.0486\n",
      "Epoch [870/2000], Loss: 0.0516\n",
      "Epoch [880/2000], Loss: 0.0523\n",
      "Epoch [890/2000], Loss: 0.0447\n",
      "Epoch [900/2000], Loss: 0.0450\n",
      "Epoch [910/2000], Loss: 0.0487\n",
      "Epoch [920/2000], Loss: 0.0485\n",
      "Epoch [930/2000], Loss: 0.0458\n",
      "Epoch [940/2000], Loss: 0.0452\n",
      "Epoch [950/2000], Loss: 0.0484\n",
      "Epoch [960/2000], Loss: 0.0486\n",
      "Epoch [970/2000], Loss: 0.0480\n",
      "Epoch [980/2000], Loss: 0.0459\n",
      "Epoch [990/2000], Loss: 0.0457\n",
      "Epoch [1000/2000], Loss: 0.0460\n",
      "Epoch [1010/2000], Loss: 0.0483\n",
      "Epoch [1020/2000], Loss: 0.0479\n",
      "Epoch [1030/2000], Loss: 0.0470\n",
      "Epoch [1040/2000], Loss: 0.0451\n",
      "Epoch [1050/2000], Loss: 0.0461\n",
      "Epoch [1060/2000], Loss: 0.0472\n",
      "Epoch [1070/2000], Loss: 0.0438\n",
      "Epoch [1080/2000], Loss: 0.0468\n",
      "Epoch [1090/2000], Loss: 0.0458\n",
      "Epoch [1100/2000], Loss: 0.0463\n",
      "Epoch [1110/2000], Loss: 0.0459\n",
      "Epoch [1120/2000], Loss: 0.0445\n",
      "Epoch [1130/2000], Loss: 0.0490\n",
      "Epoch [1140/2000], Loss: 0.0462\n",
      "Epoch [1150/2000], Loss: 0.0474\n",
      "Epoch [1160/2000], Loss: 0.0453\n",
      "Epoch [1170/2000], Loss: 0.0418\n",
      "Epoch [1180/2000], Loss: 0.0445\n",
      "Epoch [1190/2000], Loss: 0.0448\n",
      "Epoch [1200/2000], Loss: 0.0443\n",
      "Epoch [1210/2000], Loss: 0.0481\n",
      "Epoch [1220/2000], Loss: 0.0483\n",
      "Epoch [1230/2000], Loss: 0.0464\n",
      "Epoch [1240/2000], Loss: 0.0456\n",
      "Epoch [1250/2000], Loss: 0.0435\n",
      "Epoch [1260/2000], Loss: 0.0468\n",
      "Epoch [1270/2000], Loss: 0.0469\n",
      "Epoch [1280/2000], Loss: 0.0487\n",
      "Epoch [1290/2000], Loss: 0.0461\n",
      "Epoch [1300/2000], Loss: 0.0429\n",
      "Epoch [1310/2000], Loss: 0.0425\n",
      "Epoch [1320/2000], Loss: 0.0466\n",
      "Epoch [1330/2000], Loss: 0.0418\n",
      "Epoch [1340/2000], Loss: 0.0452\n",
      "Epoch [1350/2000], Loss: 0.0472\n",
      "Epoch [1360/2000], Loss: 0.0463\n",
      "Epoch [1370/2000], Loss: 0.0446\n",
      "Epoch [1380/2000], Loss: 0.0446\n",
      "Epoch [1390/2000], Loss: 0.0446\n",
      "Epoch [1400/2000], Loss: 0.0456\n",
      "Epoch [1410/2000], Loss: 0.0433\n",
      "Epoch [1420/2000], Loss: 0.0452\n",
      "Epoch [1430/2000], Loss: 0.0441\n",
      "Epoch [1440/2000], Loss: 0.0441\n",
      "Epoch [1450/2000], Loss: 0.0435\n",
      "Epoch [1460/2000], Loss: 0.0437\n",
      "Epoch [1470/2000], Loss: 0.0464\n",
      "Epoch [1480/2000], Loss: 0.0450\n",
      "Epoch [1490/2000], Loss: 0.0441\n",
      "Epoch [1500/2000], Loss: 0.0439\n",
      "Epoch [1510/2000], Loss: 0.0461\n",
      "Epoch [1520/2000], Loss: 0.0438\n",
      "Epoch [1530/2000], Loss: 0.0428\n",
      "Epoch [1540/2000], Loss: 0.0440\n",
      "Epoch [1550/2000], Loss: 0.0432\n",
      "Epoch [1560/2000], Loss: 0.0457\n",
      "Epoch [1570/2000], Loss: 0.0448\n",
      "Epoch [1580/2000], Loss: 0.0448\n",
      "Epoch [1590/2000], Loss: 0.0444\n",
      "Epoch [1600/2000], Loss: 0.0425\n",
      "Epoch [1610/2000], Loss: 0.0390\n",
      "Epoch [1620/2000], Loss: 0.0431\n",
      "Epoch [1630/2000], Loss: 0.0437\n",
      "Epoch [1640/2000], Loss: 0.0444\n",
      "Epoch [1650/2000], Loss: 0.0425\n",
      "Epoch [1660/2000], Loss: 0.0454\n",
      "Epoch [1670/2000], Loss: 0.0455\n",
      "Epoch [1680/2000], Loss: 0.0420\n",
      "Epoch [1690/2000], Loss: 0.0446\n",
      "Epoch [1700/2000], Loss: 0.0445\n",
      "Epoch [1710/2000], Loss: 0.0466\n",
      "Epoch [1720/2000], Loss: 0.0436\n",
      "Epoch [1730/2000], Loss: 0.0451\n",
      "Epoch [1740/2000], Loss: 0.0420\n",
      "Epoch [1750/2000], Loss: 0.0427\n",
      "Epoch [1760/2000], Loss: 0.0454\n",
      "Epoch [1770/2000], Loss: 0.0470\n",
      "Epoch [1780/2000], Loss: 0.0442\n",
      "Epoch [1790/2000], Loss: 0.0459\n",
      "Epoch [1800/2000], Loss: 0.0440\n",
      "Epoch [1810/2000], Loss: 0.0409\n",
      "Epoch [1820/2000], Loss: 0.0444\n",
      "Epoch [1830/2000], Loss: 0.0441\n",
      "Epoch [1840/2000], Loss: 0.0478\n",
      "Epoch [1850/2000], Loss: 0.0439\n",
      "Epoch [1860/2000], Loss: 0.0429\n",
      "Epoch [1870/2000], Loss: 0.0476\n",
      "Epoch [1880/2000], Loss: 0.0432\n",
      "Epoch [1890/2000], Loss: 0.0408\n",
      "Epoch [1900/2000], Loss: 0.0431\n",
      "Epoch [1910/2000], Loss: 0.0466\n",
      "Epoch [1920/2000], Loss: 0.0432\n",
      "Epoch [1930/2000], Loss: 0.0433\n",
      "Epoch [1940/2000], Loss: 0.0438\n",
      "Epoch [1950/2000], Loss: 0.0411\n",
      "Epoch [1960/2000], Loss: 0.0430\n",
      "Epoch [1970/2000], Loss: 0.0417\n",
      "Epoch [1980/2000], Loss: 0.0422\n",
      "Epoch [1990/2000], Loss: 0.0433\n",
      "Epoch [2000/2000], Loss: 0.0429\n"
     ]
    }
   ],
   "source": [
    "costs = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        costs.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ploting the cost-epoch\n",
    "iterations = np.arange(0, len(costs))\n",
    "with torch.no_grad():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(iterations, costs, 'b-', linewidth=2)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Cost vs. Iterations\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1425\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')  # Ensure this is the correct path to your test CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the test dataset before replacement:\n",
      "StudentID             0\n",
      "Age                   0\n",
      "Gender                0\n",
      "Ethnicity             0\n",
      "ParentalEducation    40\n",
      "StudyTimeWeekly       0\n",
      "Absences              0\n",
      "Tutoring              0\n",
      "ParentalSupport      30\n",
      "Extracurricular       0\n",
      "Sports                0\n",
      "Music                 0\n",
      "Volunteering          0\n",
      "GPA                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values in the test dataset before replacement:\")\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7688/1716585061.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['ParentalEducation'].fillna(education_mode, inplace=True)\n",
      "/tmp/ipykernel_7688/1716585061.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['ParentalSupport'].fillna(support_mode, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "test_df['ParentalEducation'].fillna(education_mode, inplace=True)\n",
    "test_df['ParentalSupport'].fillna(support_mode, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['ParentalEducation'] = pd.factorize(test_df['ParentalEducation'])[0]\n",
    "test_df['ParentalSupport'] = pd.factorize(test_df['ParentalSupport'])[0]\n",
    "test_df['Ethnicity'] = pd.factorize(test_df['Ethnicity'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = test_df.drop(['StudentID', 'GPA'], axis=1)\n",
    "y_test_new = test_df['GPA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new_scaled = scaler.transform(X_test_new)\n",
    "X_test_new_scaled = pd.DataFrame(X_test_new_scaled, columns=X_test_new.columns)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_test_new_tensor = torch.tensor(X_test_new_scaled.values, dtype=torch.float32)\n",
    "y_test_new_tensor = torch.tensor(y_test_new.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_new_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score on the test data: 0.8294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test_new_tensor.numpy(), y_test_pred.numpy())\n",
    "print(f\"R² score on the test data: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
